{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_LayerNormalization(keras.layers.Layer):\n",
    "    def build(self, batch_input_shape):\n",
    "        self.alpha = self.add_weight(name=\"alpha\",\n",
    "                                     shape=batch_input_shape[-1:],\n",
    "                                     dtype=\"float32\",\n",
    "                                     initializer=\"ones\",\n",
    "                                     trainable=True)\n",
    "        self.beta = self.add_weight(name=\"beta\",\n",
    "                                    shape=batch_input_shape[-1:],\n",
    "                                    dtype=\"float32\",\n",
    "                                    initializer=\"zeros\",\n",
    "                                    trainable=True)\n",
    "        super().build(batch_input_shape)\n",
    "    \n",
    "    def call(self, X):\n",
    "        mu, sig_sq = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        sig = tf.math.sqrt(sig_sq)\n",
    "        epsilon = tf.constant(0.001, dtype=\"float32\", name=\"epsilon\")\n",
    "        return self.alpha * (X - mu) / (sig + epsilon) + self.beta\n",
    "        \n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ln = my_LayerNormalization()\n",
    "ln = keras.layers.LayerNormalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([13, 22, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ln.compute_output_shape(tf.TensorShape([13, 22, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([13, 22, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln.compute_output_shape(tf.TensorShape([13, 22, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.build([3,3])\n",
    "my_ln.build([3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[ 0.2, 23. , 13. ],\n",
       "       [ 4. ,  5. ,  6. ],\n",
       "       [ 7. ,  8. ,  9. ]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.constant([[0.2,23,13], [4,5,6], [7,8,9]], dtype=\"float32\")\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[-1.271681  ,  1.1716609 ,  0.10001975],\n",
       "       [-1.2238274 ,  0.        ,  1.2238274 ],\n",
       "       [-1.2238274 ,  0.        ,  1.2238274 ]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln.call(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[-1.2715518 ,  1.171542  ,  0.10000969],\n",
       "       [-1.2232467 ,  0.        ,  1.2232467 ],\n",
       "       [-1.2232467 ,  0.        ,  1.2232467 ]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ln.call(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "#X_train, y_train = X_train[:3000], y_train[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=\"glorot_normal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
    "          end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "loss_fn = keras.losses.categorical_crossentropy\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.Accuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer flatten_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "60000/60000 - mean: 0.4532 - accuracy: 0.0000\n",
      "\n",
      "validation loss / accuracy: 0.47989600896835327 / 0.00013000000035390258\n",
      "Epoch 2/20\n",
      "60000/60000 - mean: 0.3449 - accuracy: 0.0004\n",
      "\n",
      "validation loss / accuracy: 0.37144985795021057 / 0.0009399999980814755\n",
      "Epoch 3/20\n",
      "60000/60000 - mean: 0.3003 - accuracy: 0.0020\n",
      "\n",
      "validation loss / accuracy: 0.3503665626049042 / 0.003370000049471855\n",
      "Epoch 4/20\n",
      "60000/60000 - mean: 0.2835 - accuracy: 0.0039\n",
      "\n",
      "validation loss / accuracy: 0.37425991892814636 / 0.005530000198632479\n",
      "Epoch 5/20\n",
      "60000/60000 - mean: 0.2629 - accuracy: 0.0066\n",
      "\n",
      "validation loss / accuracy: 0.38405683636665344 / 0.006490000057965517\n",
      "Epoch 6/20\n",
      "60000/60000 - mean: 0.2511 - accuracy: 0.0091\n",
      "\n",
      "validation loss / accuracy: 0.3373716175556183 / 0.011389999650418758\n",
      "Epoch 7/20\n",
      "60000/60000 - mean: 0.2343 - accuracy: 0.0122\n",
      "\n",
      "validation loss / accuracy: 0.3393590450286865 / 0.013159999623894691\n",
      "Epoch 8/20\n",
      "60000/60000 - mean: 0.2286 - accuracy: 0.0134\n",
      "\n",
      "validation loss / accuracy: 0.3583436608314514 / 0.01671000011265278\n",
      "Epoch 9/20\n",
      "60000/60000 - mean: 0.2186 - accuracy: 0.0146\n",
      "\n",
      "validation loss / accuracy: 0.3814762532711029 / 0.015329999849200249\n",
      "Epoch 10/20\n",
      "60000/60000 - mean: 0.2042 - accuracy: 0.0165\n",
      "\n",
      "validation loss / accuracy: 0.3448188602924347 / 0.014720000326633453\n",
      "Epoch 11/20\n",
      "60000/60000 - mean: 0.1987 - accuracy: 0.0176\n",
      "\n",
      "validation loss / accuracy: 0.3623729646205902 / 0.01844000071287155\n",
      "Epoch 12/20\n",
      "60000/60000 - mean: 0.1954 - accuracy: 0.0184\n",
      "\n",
      "validation loss / accuracy: 0.3825603425502777 / 0.0203699991106987\n",
      "Epoch 13/20\n",
      "60000/60000 - mean: 0.1865 - accuracy: 0.0210\n",
      "\n",
      "validation loss / accuracy: 0.3803015649318695 / 0.020749999210238457\n",
      "Epoch 14/20\n",
      "60000/60000 - mean: 0.1751 - accuracy: 0.0223\n",
      "\n",
      "validation loss / accuracy: 0.3706965446472168 / 0.02442999929189682\n",
      "Epoch 15/20\n",
      "60000/60000 - mean: 0.1720 - accuracy: 0.0236\n",
      "\n",
      "validation loss / accuracy: 0.37836259603500366 / 0.02386000007390976\n",
      "Epoch 16/20\n",
      "60000/60000 - mean: 0.1629 - accuracy: 0.0260\n",
      "\n",
      "validation loss / accuracy: 0.36418870091438293 / 0.02377999946475029\n",
      "Epoch 17/20\n",
      "60000/60000 - mean: 0.1576 - accuracy: 0.0280\n",
      "\n",
      "validation loss / accuracy: 0.3919597566127777 / 0.03092999942600727\n",
      "Epoch 18/20\n",
      "60000/60000 - mean: 0.1536 - accuracy: 0.0300\n",
      "\n",
      "validation loss / accuracy: 0.39727142453193665 / 0.02881000004708767\n",
      "Epoch 19/20\n",
      "60000/60000 - mean: 0.1465 - accuracy: 0.0314\n",
      "\n",
      "validation loss / accuracy: 0.3777015209197998 / 0.030230000615119934\n",
      "Epoch 20/20\n",
      "60000/60000 - mean: 0.1497 - accuracy: 0.0334\n",
      "\n",
      "validation loss / accuracy: 0.41438573598861694 / 0.03623000159859657\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    val_loss = tf.reduce_mean(loss_fn(y_test, model(X_test, training=False)))\n",
    "    val_acc = keras.metrics.Accuracy()(y_test, model(X_test, training=False))\n",
    "    print(\"\\nvalidation loss / accuracy: {} / {}\".format(val_loss, val_acc))\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = keras.optimizers.Nadam()\n",
    "optimizer2 = keras.optimizers.Adam()\n",
    "lower = keras.models.Sequential()\n",
    "upper = keras.models.Sequential()\n",
    "lower.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "lower.add(keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "upper.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "upper.add(keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=\"glorot_normal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 - mean: 0.4372 - accuracy: 0.0001\n",
      "\n",
      "validation loss / accuracy: 0.40593862533569336 / 0.0002099999983329326\n",
      "Epoch 2/20\n",
      "60000/60000 - mean: 0.3447 - accuracy: 0.0004\n",
      "\n",
      "validation loss / accuracy: 0.40674111247062683 / 0.0008399999933317304\n",
      "Epoch 3/20\n",
      "60000/60000 - mean: 0.3173 - accuracy: 0.0028\n",
      "\n",
      "validation loss / accuracy: 0.3466922342777252 / 0.005900000222027302\n",
      "Epoch 4/20\n",
      "60000/60000 - mean: 0.2890 - accuracy: 0.0066\n",
      "\n",
      "validation loss / accuracy: 0.35494741797447205 / 0.006680000107735395\n",
      "Epoch 5/20\n",
      "60000/60000 - mean: 0.2741 - accuracy: 0.0092\n",
      "\n",
      "validation loss / accuracy: 0.3858548402786255 / 0.00937000010162592\n",
      "Epoch 6/20\n",
      "60000/60000 - mean: 0.2587 - accuracy: 0.0117\n",
      "\n",
      "validation loss / accuracy: 0.36618533730506897 / 0.01119999960064888\n",
      "Epoch 7/20\n",
      "60000/60000 - mean: 0.2444 - accuracy: 0.0132\n",
      "\n",
      "validation loss / accuracy: 0.35157474875450134 / 0.014000000432133675\n",
      "Epoch 8/20\n",
      "60000/60000 - mean: 0.2391 - accuracy: 0.0139\n",
      "\n",
      "validation loss / accuracy: 0.3812224268913269 / 0.015960000455379486\n",
      "Epoch 9/20\n",
      "60000/60000 - mean: 0.2238 - accuracy: 0.0160\n",
      "\n",
      "validation loss / accuracy: 0.3638916611671448 / 0.017400000244379044\n",
      "Epoch 10/20\n",
      "60000/60000 - mean: 0.2152 - accuracy: 0.0164\n",
      "\n",
      "validation loss / accuracy: 0.3552267551422119 / 0.01696999929845333\n",
      "Epoch 11/20\n",
      "60000/60000 - mean: 0.2053 - accuracy: 0.0175\n",
      "\n",
      "validation loss / accuracy: 0.3628082871437073 / 0.018069999292492867\n",
      "Epoch 12/20\n",
      "60000/60000 - mean: 0.1978 - accuracy: 0.0192\n",
      "\n",
      "validation loss / accuracy: 0.3515720069408417 / 0.019290000200271606\n",
      "Epoch 13/20\n",
      "60000/60000 - mean: 0.1905 - accuracy: 0.0201\n",
      "\n",
      "validation loss / accuracy: 0.39378783106803894 / 0.020250000059604645\n",
      "Epoch 14/20\n",
      "60000/60000 - mean: 0.1835 - accuracy: 0.0220\n",
      "\n",
      "validation loss / accuracy: 0.37712371349334717 / 0.024250000715255737\n",
      "Epoch 15/20\n",
      "60000/60000 - mean: 0.1809 - accuracy: 0.0233\n",
      "\n",
      "validation loss / accuracy: 0.34702515602111816 / 0.025599999353289604\n",
      "Epoch 16/20\n",
      "60000/60000 - mean: 0.1670 - accuracy: 0.0252\n",
      "\n",
      "validation loss / accuracy: 0.3657115399837494 / 0.02696000039577484\n",
      "Epoch 17/20\n",
      "60000/60000 - mean: 0.1639 - accuracy: 0.0286\n",
      "\n",
      "validation loss / accuracy: 0.3798178732395172 / 0.030549999326467514\n",
      "Epoch 18/20\n",
      "60000/60000 - mean: 0.1610 - accuracy: 0.0306\n",
      "\n",
      "validation loss / accuracy: 0.41685909032821655 / 0.03536999970674515\n",
      "Epoch 19/20\n",
      "60000/60000 - mean: 0.1531 - accuracy: 0.0326\n",
      "\n",
      "validation loss / accuracy: 0.36910420656204224 / 0.033070001751184464\n",
      "Epoch 20/20\n",
      "60000/60000 - mean: 0.1469 - accuracy: 0.0367\n",
      "\n",
      "validation loss / accuracy: 0.39538511633872986 / 0.04030999913811684\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train, y_train)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            low = lower(X_batch, training=True)\n",
    "            y_pred = upper(low, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + upper.losses + lower.losses)\n",
    "        gradients_upper = tape.gradient(loss, upper.trainable_variables)\n",
    "        optimizer1.apply_gradients(zip(gradients_upper, upper.trainable_variables))\n",
    "        gradients_lower = tape.gradient(loss, lower.trainable_variables)\n",
    "        del tape\n",
    "        optimizer2.apply_gradients(zip(gradients_lower, lower.trainable_variables))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    val_loss = tf.reduce_mean(loss_fn(y_test, upper(lower(X_test, training=False), training=False)))\n",
    "    val_acc = keras.metrics.Accuracy()(y_test, upper(lower(X_test, training=False), training=False))\n",
    "    print(\"\\nvalidation loss / accuracy: {} / {}\".format(val_loss, val_acc))\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
